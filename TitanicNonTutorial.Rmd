## The Kaggle Titanic Competition - a non-tutorial

Kaggle has some nice competitions to work on in order to get some experience with machine learning so
I've been using it as an opportunity to test some of the ideas I've been studying in a few data 
science/machine learning courses on [Coursera](http://coursera.org).

 - University of Washington [Introduction to Data Science](https://class.coursera.org/datasci-002)
 - Stanford University [Machine Learning](https://class.coursera.org/ml-006)
 - John Hopkins University [Data Science specialisation](https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage)

I found these to be really good courses, well worth following, all a bit different in approach and style.

This is a very much tidied-up version of the work I did for the Titanic competition - a "ML-101" problem; 
all of the cul-de-sacs, wrong turns and explorations have been omitted in the interests of presenting a reasonable narrative. 

## The problem

Given a training data set of passengers on the Titanic consisting of various features (Age, Sex, Passenger class, ...) and an indicator of whether or not they survived the disaster, predict the survival of passengers in a test data. The competition is judged (as far as I can see) on the *Accuracy* of the predictions, that is the percentage of correctly classified records.

## Setup the environment

Load the necessary packages
```{r results="hide"}
library(ggplot2)
library(reshape2)    # To reshape date for ggplot
library(e1071)       # SVM implementation
```

Read both training data and test data
```{r}
df <- read.csv('train.csv')
df.test <- read.csv('test.csv')
```

## Combine the two data sets 
From a preliminary investigation we know that age information is missing for a large number 
of passengers, I will use the whole data set in order to try to model an estimator for
missing values (imputation). In order to establish family links between the passengers for
the creation of new features I'll need to use the combined data sets as there is no guarentee 
(and in fact it is not the case) that the training/test partiion respects family ties.

Data will be separable afterwards via the test `is.na(Survived)`. 
```{r}
df.test$Survived <- NA
df <- rbind(df, df.test)
```

## Check the data

Before attempting any analysis on the data we need to check its cleanliness.

- Look for missing `NA` values
```{}
missing.data <- sapply(colnames(df), function(x) {any(is.na(df[x]))})
names(missing.data)[missing.data]
```

There is one `NA` value for `Fare` I just replace it with the mean value
```{r}
df$Fare[is.na(df$Fare)] <- mean(df$Fare, na.rm=TRUE)
```

- Look for incorrect data

There are two entries in the data Which have `""` for one of `Embarked` field,
I set them both to `"C"` Based on the price paid.
```{r}
df$Embarked[df$Embarked == ""] <- "C" 
```

- Visualise the data 


## Creation of new features

After having played around with the data a bit I looked for ways to squeeze some more information
out of the data. Parsing some of the data fields is one way to obtain new features as is looking 
at relations between different passengers. In this way I extract two additional feature:

 - Title (useful for imputing age)
 - RelatedSurvivors (thinking that families would tend to stick together)
 
`Title` is obtained from splitting off second element after splitting on `", "` and `"."`.
Rare titles are replaced by "guess work" to leave the levels: "Mr", "Mrs", "Master", "Miss"
```{r}
extractTitle <- function(Names) {
  Title  <- sapply(Names, function(x) strsplit(as.character(x),'\\.|, ')[[1]][[2]])
  Title[is.element(Title, c("Capt", "Col", "Don", "Dr", "Jonkheer", "Major", "Rev", "Sir"))] <- "Mr"
  Title[is.element(Title, c("Lady", "Mme", "Ms", "Dona", "the Countess"))] <- "Mrs"
  Title[is.element(Title, c("Mlle"))] <- "Miss"
  
  Title <- factor(Title, levels=c("Mr","Master","Mrs","Miss"))
}

df$Title <- extractTitle(df$Name)
``` 

Extract the family name; it is the first element of the `Name` feature
```{r}
extractFamily <- function(Names) {
  Title  <- sapply(Names, function(x) strsplit(as.character(x),'\\.|, ')[[1]][[1]])
}

df$Family <- extractFamily(df$Name)
```

The family name will not be used directly in the survival predictions but it is used
to form family groups. Information on whether a passenger had surviving relatives
is relevant. For example within a subset of passengers for which all information is known,
P(Survival) = 0.42 whereas P(Survival | Surviving Relative) = 0.59
** Put in code that shows this **

The following function tries to define for each passenger whether or not another family 
member is known to have survived. `x` is a `data.frame` of members of the same family 
(based on family name) with the same columns as per the raw data frame. The function returns 
this `data.frame` with an additional column `RelatedSurvivors`.
Each passenger is classed as:
  - `NoFamily`  The passenger had no family aboard
  - `Unknown`   It is unknown whether a family member survived
  - `None`      It is known that no family member survived
  - `Some`      It is known that at least one family member survived

```{r}
extractSurvivingRelatives <- function(x) {
  classify <- function(p) {
    if ( any(is.na(p$Survived) ) ) {
      class <- 'Unknown'
    } else if ( all(p$Survived == 0) ) {
      class <- 'None'
    } else if ( any(p$Survived == 1) ) {
      class <- 'Some'
    } 
    class
  }
  
  if ( nrow(x) == 1 ) {
    # Passenger travelling without family (survived maybe NA or not)
    RelatedSurvivors <- 'NoRelatedFamily'
  } else {
    ## I'd love to see how to vectorize this !
    RelatedSurvivors <- character(nrow(x))
    for ( i in 1:nrow(x) ) {
      RelatedSurvivors[i] <- classify(x[-i,])
    }
  }
  
  x$RelatedSurvivors <- factor(RelatedSurvivors, levels=c('NoRelatedFamily', 'Unknown', 'None', 'Some'))
  x
}
```

The function is used to update the data as follows:

 - Group into families by name (obviously not perfect)
 - Apply the "related survivors algorithm"
 - Recombine into a single data frame and replace `df` 

```{r}
CFG <- lapply(unique(df$Family), function(x,D) df[df$Family == x,], D=df)
CFG.rs <- lapply(CFG, extractSurvivingRelatives)
CFG.rs <- Reduce(function(x,x0) rbind(x,x0), CFG.rs[-1], CFG.rs[1][[1]])
df <- CFG.rs
```

## More data manipulation

I re-express `Pclass` as categorical and re-factor `Embarked` because one
of the levels is no longer present. Factoring the passenger class looked
like a good idea at the time. I would look into again now to see if it is 
a good idea.
```{r}
df$Pclass <- factor(df$Pclass, levels=1:3)
df$Embarked <- factor(df$Embarked)
```

## Imputation of missing age values

Age is *the* feature for which there are many missing values and whose value
is a clear determinant in survival. Rather than fill in missing values with
a simple mean I regress the age over some of the other variables in an attempt
to estimate it more accurately

Split the data according as to whether the `age` has a value and use a linear
regression to extract a model of `Age` 
```{r}
df.withoutAge <- df[is.na(df$Age),]
df.withAge <- df[!is.na(df$Age),]
age.model <- lm(Age ~ Sex + Pclass + SibSp + Parch + Fare + Title, data=df.withAge)
```

The predictions of `Age` given by the model compared to the known value is shown 
below. Since it is not obvious nonsense I'll use this model to predict the missing 
`Age` values in both the training and the test data sets.
```{r "fig_age", fig.width=6, fig.height=4}
ggplot(data=cbind(df.withAge,PredictedAge=fitted(age.model))) + 
                    geom_point(aes(x=Age, y=PredictedAge, colour=Title)) +
                    labs(title='Age imputation', x='Known age from data', y='Predicted age')
```

Set the Age feature wherever it is `NA` 
```{r}
df.withoutAge$Age <- predict(age.model, newdata=df.withoutAge)
```

## Reorganize data
I now recombine the split dataset and then separate out the test and training sets 
```{r}
df <- rbind(df.withAge, df.withoutAge)

df.train <- df[!is.na(df$Survived),]
df.test <- df[is.na(df$Survived),]
```
Define `Survived` as a factor and remove from it the test data
```{r}
df.train$Survived <- factor(df.train$Survived)
df.test$Survived <- NULL
```

We have now done all the data manipulation necessary and re-split the data into
a training set and a test set.

## Establish a base case
- Naive Bayes probabilities
- Base case

 ** To be added **

## SVM Training

I train a support vector machine with a Gaussian kernel to classify the data. This 
requires the definition of two parameters: C and Gamma. As for the model, I take 
all the features including those derived above:
```{r}
F <- Survived ~ Sex + Pclass + poly(Age,3) + SibSp + Parch + Fare + Title + Embarked + RelatedSurvivors
```

Optimal values for C (Regularization) and Gamma (1/sigma) can be found by performing a grid
search. This is a "logarithmic" grid, once we find an optimal value we could do a linear grid
search to focus in better on the optimal value but here I don't.
```{r}
obj <- tune(svm, F, data = df.train, 
            ranges = list(gamma = 2^(-8:-1), cost = 2^(-2:9)),
            tunecontrol = tune.control(sampling = "cross", cross=5)
)
gamma.opt <- obj$best.parameters$gamma
cost.opt <- obj$best.parameters$cost
```
This optimisation splits the data into training and cross-validation sets (number of partitions 
specified by `cross`) and gives C=`r cost.opt` and Gamma=`r gamma.opt` as optimal parameters to 
use. 

I want to split the data into training and cross-validation sets and then train the SVM on
increasing subsets of the data so as to be able to plot a learning curve of sorts. 
The following function takes a training set and a cross validation set and:

 - Trains the SVM
 - Predicts `Survived` for both training and cross validation data sets
 - Calculates the accuracy for both data sets

```{r}
trainAndCrossValidate <- function(df.train, df.cv) {  
  model <- svm(F, gamma=gamma.opt, cost=cost.opt, data=df.train)
  
  ## Apply to Training Data
  predictions <- predict(model, newdata=df.train)
  
  ## Statistics for training set
  accuracy.train <- length(which(df.train$Survived == predictions)) / nrow(df.train)
  
  ## Apply to cross-validation data
  predictions <- predict(model, newdata=df.cv)
  accuracy.cv <- length(which(df.cv$Survived == predictions)) / nrow(df.cv)
  
  return(list(accuracy.train=accuracy.train, accuracy.cv=accuracy.cv))
}
```

Randomly split the data into Training data and Cross-validation sets (Setting the 
seed for reproducibility):
```{r}
set.seed(19670122)
f <- sample(1:nrow(df.train), 200)
df.cv <- df.train[f,]
df.train <- df.train[-f,]
```

We now train and cross-validate on increasing fractions of the training data
```{r}
data.fraction <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)
#data.fraction <- c(0.1,  1)
N <- length(data.fraction)
learningCurve <- data.frame(data.fraction, accuracy.train=rep(0,N), accuracy.cv=rep(0,N))

for ( i in 1:N ) {
  cat("Training on ", data.fraction[i] * 100, "% of data\n", sep="")
  results <- trainAndCrossValidate(df.train[1:round(nrow(df.train) * data.fraction[i]),], df.cv)
  cat("   Accuracy (Training): ", results$accuracy.train * 100, "%\n", sep="")
  cat("   Accuracy (cv):       ", results$accuracy.cv * 100, "%\n", sep="")
  learningCurve$accuracy.train[i] <- results$accuracy.train
  learningCurve$accuracy.cv[i] <- results$accuracy.cv
}
```

and plot the learning curves (here I use `melt` to put the data into a form usable bby `ggplot`)
```{r "fig_learning", fig.width=6, fig.height=4}
lc <- melt(learningCurve, id.vars="data.fraction")
ggplot(data=lc) + geom_line(aes(x=data.fraction, y=value, group=variable, colour=variable)) + 
    labs(y = 'Accuracy', x = 'Fraction of training set used', title = 'Learning Curves')
```

## Performance and result

Define a function for calculating test statistics
```{r}
test_statistics <- function(labels, predictions) {
  ## Explicit factor to ensure coverage 0f {0,1} .. e.g. if all predictions = 1
  confusion_matrix <- table(data.frame(predicted=factor(predictions, levels=c(1,0)),
                                       actual=factor(labels, levels=c(1,0)) ))
  tp <- confusion_matrix[1,1]
  fp <- confusion_matrix[1,2]
  fn <- confusion_matrix[2,1]
  tn <- confusion_matrix[2,2]
  
  accuracy <- (tp + tn) / (tp + fp + fn + tn)
  precision <- tp / (tp + fp)
  recall <- tp / (tp + fn)
  F <- 2 * precision * recall / (precision + recall)
  
  # Return stats in a list
  list(confusion_matrix=confusion_matrix, accuracy=accuracy, precision=precision, recall=recall, F1=F)
}
```
Calculate the confusion matrix for the training data
```{r results="hide"}
  model <- svm(F, gamma=gamma.opt, cost=cost.opt, data=df.train)
  predictions <- predict(model, newdata=df.train)

  stats <- test_statistics(df.train$Survived, predictions)
```

So the resulting confusion matrix looks like this:
```{r}
stats$confusion_matrix
```
with statistics:

 - Accuracy:  `r stats$accuracy`
 - Precision: `r stats$precision`
 - Recall:    `r stats$recall`
 - F1:        `r stats$F1`

And for the cross validation data
```{r results="hide"}
  model <- svm(F, gamma=gamma.opt, cost=cost.opt, data=df.cv)
  predictions <- predict(model, newdata=df.cv)

  stats <- test_statistics(df.cv$Survived, predictions)
```

the resulting confusion matrix looks like this:
```{r}
stats$confusion_matrix
```
with statistics:

 - Accuracy:  `r stats$accuracy`
 - Precision: `r stats$precision`
 - Recall:    `r stats$recall`
 - F1:        `r stats$F1`


## Produce predictions for test data set 

First I retrain the model on the full data set with the optimal parameters previously determined.
I don't think this is a good idea in general since we now lose our estimate of the algorithms 
performance, but since the aim is to creep up the leader board a little bit, why not?

```{r}
df$Survived <- factor(df$Survived)
model <- svm(F, data=df[!is.na(df$Survived),], gamma=gamma.opt, cost=cost.opt)
```

Now apply the model to the test data and write the predictions into a file that may be directly 
uploaded to [Kaggel](https://www.kaggle.com/).

```{r}
predictions <- predict(model, newdata=df.test)

write.table(data.frame(PassengerId=df.test$PassengerId, Survived=predictions), 
            file="test_predictions.csv", row.names=FALSE, sep=",")
```

## Conclusions
This should correspond exactly to the code that I used to get two thirds of the way up the leader board. 
There's plenty of scope for playing around with what I've done and I'm sure many ways to improve it. 

The test data set consists of 418 passengers, so every additional passenger correctly classified gives you an 
additional 0.2% in accuracy. The real thing to understand at this point is whether the svm solution as it
stands is limited by high bias or high variance. I'm still thinking about this, and frankly am not sure 
which of these is the case. Clearly there is no more data available, but maybe there are more features 
that may be extracted from the data, which would be useful if the model has high bias. The fact that the 
accuracy of the cross validation data set is not decreasing with increasing data leads me to think that the
model generalises well and so is not over-fitting.

Another thing I've been thinking about is using a decision tree algorithm as well and thinking about whether 
by combining  the results of the two approaches I can get higher accuracy. Time, however, is unfortunately 
in short supply.


## R and package versions used

```{r sessionInfo, include=TRUE, echo=TRUE, results='markup'}
sessionInfo()
```
